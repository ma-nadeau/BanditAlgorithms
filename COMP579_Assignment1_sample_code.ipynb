{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKkeYE20a37b"
      },
      "source": [
        "# COMP 579 Assignment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Do Not Change the Random Seed\n",
        "The random seed has been set to ensure reproducibility. Please do not modify it.\n",
        "\n",
        "2. Guidance for the First Question\n",
        "For the initial question, fill in the blanks under the sections marked as TODO. Follow the provided structure and complete the missing parts.\n",
        "\n",
        "3. Approach for Subsequent Questions\n",
        "For the later questions, we expect you to attempt the solutions independently. You can refer to the examples provided in earlier questions to understand how to \n",
        "plot figures and implement solutions.\n",
        "\n",
        "4. Ensure that the plots you produce for later questions are similar in style and format to those shown in the previous examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eEvd8WcFqvai"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "np.random.seed(40)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"]=10,5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss77N5TbLVIl"
      },
      "source": [
        "## Q1 Simulator for Bernoulli Bandit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "id": "uG8suY4Sn7hu"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GaussianBandit:\n",
        "  \"\"\"\n",
        "    A class representing a Gaussian multi-armed bandit.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    num_arms : int\n",
        "        Number of arms in the bandit.\n",
        "    mean : list or np.ndarray\n",
        "        List of mean rewards for each arm.\n",
        "    variance : float\n",
        "        Variance of the rewards for all arms.\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    sample(arm_index)\n",
        "        Samples a reward from the specified arm based on a Gaussian distribution.\n",
        "    \"\"\"\n",
        "\n",
        "  # TODO:\n",
        "  def __init__(self, num_arms, mean, variance):\n",
        "    self.num_arms = \n",
        "    self.mean = \n",
        "    self.variance = \n",
        "\n",
        "  def sample(self, arm_index):\n",
        "    return\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "id": "A2G0G_s5sy_C"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "delta = \n",
        "num_arms = \n",
        "means = \n",
        "variance = \n",
        "num_samples = \n",
        "\n",
        "three_arm_gaussian_bandit =\n",
        "\n",
        "# Store the rewards for each arm\n",
        "action_rewards = []\n",
        "actions = range(num_arms)\n",
        "\n",
        "for action in actions:\n",
        "    # Store 50 samples per action\n",
        "    rewards = \n",
        "    action_rewards.append(rewards)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG_coTYL1_RL"
      },
      "source": [
        "### Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pj04OZ0ozUjK",
        "outputId": "ee09b4df-c2f2-4d98-8146-0c2e184b64c7"
      },
      "outputs": [],
      "source": [
        "for action in actions:\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # TODO:\n",
        "  true_value = \n",
        "  estimated_value = \n",
        "\n",
        "  # draw the line of the true value\n",
        "  line_true_val = ax.axhline(y = true_value, color = 'b', linestyle = ':', label = \"true value\")\n",
        "  # draw the line of the estimated value\n",
        "  line_est_val = ax.axhline(y = estimated_value, color = 'r', linestyle = '--', label = \"estimated value\")\n",
        "  # plot the reward samples\n",
        "  plt_samples, = ax.plot(action_rewards[action], 'o', label = \"reward samples\")\n",
        "\n",
        "  ax.set_xlabel(\"sample number\")\n",
        "  ax.set_ylabel(\"reward value\")\n",
        "  ax.set_title(\"Sample reward, estimated and true expected reward over 50 samples for action %s\" %action, y=-0.2)\n",
        "\n",
        "  # show the legend with the labels of the line\n",
        "  ax.legend(handles=[line_true_val, line_est_val, plt_samples])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKLg6U5bLhRs"
      },
      "source": [
        "## Q2 Estimated Q values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "id": "cgnKe19V0NgR"
      },
      "outputs": [],
      "source": [
        "def update(reward_samples, alpha):\n",
        "  \"\"\"\n",
        "  Each call to the function yields the current incremental average of the reward with a fixed learning rate, alpha\n",
        "  E.g. Inital call returns alpha * reward_samples[0], second call returns prev_val + alpha * (reward_samples[1] - prev_val)\n",
        "  where prev_val is the value return from the previous call, so on and so forth\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  reward_samples : array of int\n",
        "      samples of reward values from one arm of a bandit\n",
        "  alpha : int\n",
        "      learning rate parameter for the averaging\n",
        "  \"\"\"\n",
        "\n",
        "    yield \n",
        "\n",
        "def updateAvg(reward_samples):\n",
        "  \"\"\"\n",
        "  Each call to the function yields the current incremental average of the reward\n",
        "  E.g. Inital call returns reward_samples[0], second call returns the average of reward_samples[0] and reward_samples[0], so on and so forth\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  reward_samples : array of int\n",
        "      samples of reward values from one arm of a bandit\n",
        "  \"\"\"\n",
        "\n",
        "    yield \n",
        "    \n",
        "def updateDecaying(reward_samples, alpha_0=0.5, lambda_=0.01, p=0.5):\n",
        "    \"\"\"\n",
        "    Each call to the function yields the updated estimate of the action value using an\n",
        "    improved decaying learning rate.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    reward_samples : array-like of int or float\n",
        "        Samples of reward values from one arm of a bandit.\n",
        "    alpha_0 : float, optional\n",
        "        The initial learning rate (default is 0.5).\n",
        "    lambda_ : float, optional\n",
        "        The decay rate constant (default is 0.01).\n",
        "    p : float, optional\n",
        "        The power parameter for controlling decay (default is 0.5).\n",
        "    \"\"\"\n",
        "\n",
        "        yield \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKmG74R11x-j"
      },
      "source": [
        "### Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G_ltKRTcBaDM",
        "outputId": "5e979ea2-7cb2-4978-856d-71a3899b106e"
      },
      "outputs": [],
      "source": [
        "for action in actions:\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # TODO:\n",
        "  incr_avgs = \n",
        "  alpha_1_percent = \n",
        "  alpha_10_percent = \n",
        "  alpha_decay = \n",
        "  true_value = \n",
        "\n",
        "  # draw the true value line\n",
        "  line_true_val = ax.axhline(y = true_value, color = 'b', linestyle = ':', label = \"true value\")\n",
        "\n",
        "  # plot incremental values for averaging, alpha = 0.01, alpha = 0.1\n",
        "  plt_incr_avgs, = ax.plot(incr_avgs, label = \"incremental average\")\n",
        "  plt_alpha_1_percent, = ax.plot(alpha_1_percent, label = r\"$\\alpha = 0.01$\")\n",
        "  plt_alpha_10_percent, = ax.plot(alpha_10_percent, label = r\"$\\alpha = 0.1$\")\n",
        "  plt_alpha_decay, = ax.plot(alpha_decay, label = r\"$\\alpha decay$\")\n",
        "\n",
        "  ax.set_xlabel(\"sample number\")\n",
        "  ax.set_ylabel(\"reward value\")\n",
        "  ax.set_title(\"Incremental estimates and true expected reward values over 50 samples for action %s\" %action, y=-0.2)\n",
        "\n",
        "  # show the legend with the labels of the line\n",
        "  ax.legend(handles=[line_true_val, plt_incr_avgs, plt_alpha_1_percent, plt_alpha_10_percent, plt_alpha_decay])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMdCH4-lLw44"
      },
      "source": [
        "## Q3 Effect of $Î±$ on Estimated Q values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6_tT59BB1Ro"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "num_samples = \n",
        "\n",
        "# arrays of the data generated from 100 runs\n",
        "incr_avgs_runs = []\n",
        "alpha_1_percent_runs = []\n",
        "alpha_10_percent_runs = []\n",
        "alpha_decay_runs = []\n",
        "\n",
        "\n",
        "# TODO:\n",
        "for run in range(100):\n",
        "  # arrays of data generated from the 3 actions in 1 run\n",
        "  sample_incr_avgs_by_actions = []\n",
        "  sample_alpha_1_percent_by_actions = []\n",
        "  sample_alpha_10_percent_by_actions = []\n",
        "  sample_alpha_decay_by_actions = []\n",
        "\n",
        "  for action in actions:\n",
        "    rewards = \n",
        "    sample_incr_avgs_by_actions.append()\n",
        "    sample_alpha_1_percent_by_actions.append()\n",
        "    sample_alpha_10_percent_by_actions.append()\n",
        "    sample_alpha_decay_by_actions.append()\n",
        "\n",
        "  incr_avgs_runs.append(sample_incr_avgs_by_actions)\n",
        "  alpha_1_percent_runs.append(sample_alpha_1_percent_by_actions)\n",
        "  alpha_10_percent_runs.append(sample_alpha_10_percent_by_actions)\n",
        "  alpha_decay_runs.append(sample_alpha_decay_by_actions)\n",
        "\n",
        "# convert to np arrays\n",
        "incr_avgs_runs = np.asarray(incr_avgs_runs)\n",
        "alpha_1_percent_runs = np.asarray(alpha_1_percent_runs)\n",
        "alpha_10_percent_runs = np.asarray(alpha_10_percent_runs)\n",
        "alpha_decay_runs = np.asarray(alpha_decay_runs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaDSMygu2IZc"
      },
      "source": [
        "### Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EUB4pwXaRM_E",
        "outputId": "c5b71118-9fad-4962-e00d-086f3306da53"
      },
      "outputs": [],
      "source": [
        "for action in actions:\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # obtain averaged incremental reward values for averaging, alpha = 0.01, alpha = 0.1 and decay alpha over 100 runs\n",
        "  # TODO:\n",
        "  mean_incr_avgs_by_actions = \n",
        "  mean_alpha_1_percent_by_actions = \n",
        "  mean_alpha_10_percent_by_actions =\n",
        "  mean_alpha_decay_by_actions = \n",
        "\n",
        "  true_value =\n",
        "\n",
        "  # obtain the standard deviation for averaging, alpha = 0.01, alpha = 0.1 and decay alpha over 100 runs\n",
        "  std_incr_avgs_by_actions = \n",
        "  std_alpha_1_percent_by_actions = \n",
        "  std_alpha_10_percent_by_actions = \n",
        "  std_alpha_decay_by_actions = \n",
        "\n",
        "  # obtain the standard error for averaging, alpha = 0.01, alpha = 0.1 and decay alpha over 100 runs\n",
        "  std_err_incr_avgs_by_actions = \n",
        "  std_err_alpha_1_percent_by_actions = \n",
        "  std_err_alpha_10_percent_by_actions =\n",
        "  std_err_alpha_decay_by_actions = \n",
        "  \n",
        "  # draw the true value line\n",
        "  line_true_val = ax.axhline(y = true_value, color = 'b', linestyle = ':', label = \"true value\")\n",
        "\n",
        "  # draw the averaged incremental reward values for averaging\n",
        "  plt_incr_avgs, = ax.plot(mean_incr_avgs_by_actions, label = \"incremental average\")\n",
        "  # draw the error bar/area for averaging\n",
        "  incr_avgs_minus_std_err = mean_incr_avgs_by_actions - std_err_incr_avgs_by_actions\n",
        "  incr_avgs_plus_std_err = mean_incr_avgs_by_actions + std_err_incr_avgs_by_actions\n",
        "  ax.fill_between(range(0,100), incr_avgs_minus_std_err, incr_avgs_plus_std_err, alpha=0.3)\n",
        "\n",
        "  # draw the averaged incremental reward values for alpha = 0.01\n",
        "  plt_alpha_1_percent, = ax.plot(mean_alpha_1_percent_by_actions, label = \"alpha = 0.01\")\n",
        "  # draw the error bar/area for alpha = 0.01\n",
        "  alpha_1_percent_minus_std_err = mean_alpha_1_percent_by_actions - std_err_alpha_1_percent_by_actions\n",
        "  alpha_1_percent_plus_std_err = mean_alpha_1_percent_by_actions + std_err_alpha_1_percent_by_actions\n",
        "  ax.fill_between(range(0,100), alpha_1_percent_minus_std_err, alpha_1_percent_plus_std_err, alpha=0.3)\n",
        "\n",
        "  # draw the averaged incremental reward values for alpha = 0.1\n",
        "  plt_alpha_10_percent, = ax.plot(mean_alpha_10_percent_by_actions, label = \"alpha = 0.1\")\n",
        "  # draw the error bar/area for alpha = 0.1\n",
        "  alpha_10_percent_minus_std_err = mean_alpha_10_percent_by_actions - std_err_alpha_10_percent_by_actions\n",
        "  alpha_10_percent_plus_std_err = mean_alpha_10_percent_by_actions + std_err_alpha_10_percent_by_actions\n",
        "  ax.fill_between(range(0,100), alpha_10_percent_minus_std_err, alpha_10_percent_plus_std_err, alpha=0.3)\n",
        "  \n",
        "  plt_alpha_decay, = ax.plot(mean_alpha_decay_by_actions, label = \"alpha decay\")\n",
        "  alpha_decay_minus_std_err = mean_alpha_decay_by_actions - std_err_alpha_10_percent_by_actions\n",
        "  alpha_decay_plus_std_err = mean_alpha_decay_by_actions + std_err_alpha_10_percent_by_actions\n",
        "  ax.fill_between(range(0,100), alpha_decay_minus_std_err, alpha_decay_plus_std_err, alpha=0.3)\n",
        "\n",
        "  ax.set_xlabel(\"sample number\")\n",
        "  ax.set_ylabel(\"reward value\")\n",
        "  ax.set_title(\"Incremental estimates and true expected reward values averaged over 100 runs for action %s\" %action, y=-0.2)\n",
        "\n",
        "  ax.legend(handles=[line_true_val, plt_incr_avgs, plt_alpha_1_percent, plt_alpha_10_percent, plt_alpha_decay])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HKeI_j9cpvs"
      },
      "source": [
        "### Answers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL_sU0R5L2se"
      },
      "source": [
        "## Q4 Epsilon-greedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "hEhRJLpKdhK0"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy(bandit, epsilon, alpha = None, num_time_step = 1000, epsilon_decay=False, lambda_=0.001):\n",
        "  \"\"\"Epsilon greedy algorithm for bandit action selection\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  bandit : bandit class\n",
        "      A bernoulli bandit attributes num_arms and probs_arr, and method sample\n",
        "  epsilon: float\n",
        "      A parameter which determines the probability for a random action to be selected\n",
        "  alpha: (optional) float\n",
        "      A parameter which determined the learning rate for averaging. If alpha is none, incremental averaging is used.\n",
        "      Default is none, corresponding to incremental averaging.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  R_over_t\n",
        "      a list of instantaneous return over the time steps\n",
        "  total_R_over_t\n",
        "      a list of cummulative reward over the time steps\n",
        "  est_is_best_over_t\n",
        "      a list of values of 0 and 1 where 1 indicates the estimated best action is the true best action and 0 otherwise for each time step\n",
        "  l_over_t\n",
        "      a list of instanteneous regret over the time steps\n",
        "  total_l_over_t\n",
        "      a list of cummulative regret over the time steps\n",
        "  \"\"\"\n",
        "  # TODO:\n",
        "  num_arms = \n",
        "\n",
        "  Q_arr = \n",
        "  N_arr = \n",
        "  total_R = \n",
        "  total_l = \n",
        "  actions = \n",
        "\n",
        "  opt_value = \n",
        "  best_action = \n",
        "\n",
        "  R_over_t = []\n",
        "  total_R_over_t = []\n",
        "  est_is_best_over_t = []\n",
        "  l_over_t = []\n",
        "  total_l_over_t = []\n",
        "  \n",
        "  epsilon_t = epsilon \n",
        "\n",
        "  for time_step in range(num_time_step):\n",
        "    if epsilon_decay:\n",
        "        epsilon_t = \n",
        "            \n",
        "    A_star = \n",
        "    A_random = \n",
        "    A = \n",
        "    curr_R = \n",
        "    N_arr[A] = \n",
        "\n",
        "    if alpha == None:\n",
        "      # incremental averaging\n",
        "      Q_arr[A] = \n",
        "    else:\n",
        "      Q_arr[A] = \n",
        "\n",
        "    R_over_t.append(curr_R)\n",
        "\n",
        "    total_R = \n",
        "    total_R_over_t.append(total_R)\n",
        "\n",
        "    est_is_best = \n",
        "    est_is_best_over_t.append(est_is_best)\n",
        "\n",
        "    l_t = \n",
        "    l_over_t.append(l_t)\n",
        "\n",
        "    total_l = \n",
        "    total_l_over_t.append(total_l)\n",
        "\n",
        "  return R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU1_pP7INeBH"
      },
      "source": [
        "### Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JgvBlfQDBJdv",
        "outputId": "ab9398f1-bbd5-4cbd-ffa6-c56f775a1cef"
      },
      "outputs": [],
      "source": [
        "#TODO:\n",
        "epsilons = []\n",
        "decaying_epsilon_params = {'epsilon_0': , 'lambda_': }  # Decaying epsilon parameters\n",
        "\n",
        "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(18, 18))\n",
        "\n",
        "for epsilon in epsilons + [\"decay\"]:\n",
        "\n",
        "  # arrays of the data generated from 100 runs\n",
        "  R_over_t_runs = []\n",
        "  total_R_over_t_runs = []\n",
        "  est_is_best_over_t_runs = []\n",
        "  l_over_t_runs = []\n",
        "  total_l_over_t_runs = []\n",
        "\n",
        "  for run in range(100):\n",
        "    if epsilon == \"decay\":\n",
        "      R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = epsilon_greedy(\n",
        "          three_arm_gaussian_bandit, \n",
        "          decaying_epsilon_params['epsilon_0'], \n",
        "          epsilon_decay=True, \n",
        "          lambda_=decaying_epsilon_params['lambda_']\n",
        "      )\n",
        "    else:\n",
        "      R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = epsilon_greedy(\n",
        "          three_arm_gaussian_bandit, \n",
        "          epsilon\n",
        "      )\n",
        "    R_over_t_runs.append(R_over_t)\n",
        "    total_R_over_t_runs.append(total_R_over_t)\n",
        "    est_is_best_over_t_runs.append(est_is_best_over_t)\n",
        "    l_over_t_runs.append(l_over_t)\n",
        "    total_l_over_t_runs.append(total_l_over_t)\n",
        "\n",
        "  R_over_t_runs = np.asarray(R_over_t_runs)\n",
        "  total_R_over_t_runs = np.asarray(total_R_over_t_runs)\n",
        "  est_is_best_over_t_runs = np.asarray(est_is_best_over_t_runs)\n",
        "  l_over_t_runs = np.asarray(l_over_t_runs)\n",
        "  total_l_over_t_runs = np.asarray(total_l_over_t_runs)\n",
        "\n",
        "  # plot the mean reward over time\n",
        "\n",
        "  mean_R_over_t_runs = np.mean(R_over_t_runs, axis=0)\n",
        "  std_err_R_over_t_runs = np.std(R_over_t_runs, axis=0) / np.sqrt(np.size(R_over_t_runs, axis=0))\n",
        "\n",
        "  axs[0,0].plot(mean_R_over_t_runs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
        "\n",
        "  R_over_t_minus_std_err = mean_R_over_t_runs - std_err_R_over_t_runs\n",
        "  R_over_t_plus_std_err = mean_R_over_t_runs  + std_err_R_over_t_runs\n",
        "  axs[0,0].fill_between(range(0,1000), R_over_t_minus_std_err, R_over_t_plus_std_err, alpha=0.4)\n",
        "  # axs[0,0].errorbar(range(0,1000), mean_R_over_t_runs, yerr=std_err_R_over_t_runs)\n",
        "\n",
        "  axs[0,0].legend()\n",
        "  axs[0,0].set_xlabel(\"time step\")\n",
        "  axs[0,0].set_ylabel(\"reward value\")\n",
        "  axs[0,0].set_title(\"Average Instanteneous Reward Received over Time\", y=-0.18)\n",
        "\n",
        "  # plot the mean cummulative reward over time\n",
        "\n",
        "  mean_total_R_over_t_runs = np.mean(total_R_over_t_runs, axis=0)\n",
        "  std_err_total_R_over_t_runs = np.std(total_R_over_t_runs, axis=0) / np.sqrt(np.size(total_R_over_t_runs, axis=0))\n",
        "\n",
        "  axs[0,1].plot(mean_total_R_over_t_runs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
        "\n",
        "  total_R_over_t_minus_std_err = mean_total_R_over_t_runs - std_err_total_R_over_t_runs\n",
        "  total_R_over_t_plus_std_err = mean_total_R_over_t_runs  + std_err_total_R_over_t_runs\n",
        "  axs[0,1].fill_between(range(0,1000), total_R_over_t_minus_std_err, total_R_over_t_plus_std_err, alpha=0.4)\n",
        "\n",
        "  axs[0,1].legend()\n",
        "  axs[0,1].set_xlabel(\"time step\")\n",
        "  axs[0,1].set_ylabel(\"reward value\")\n",
        "  axs[0,1].set_title(\"Average Cummulative Reward Received over Time\", y=-0.18)\n",
        "\n",
        "  #plot the mean percentage of the estimated best action being the first action\n",
        "\n",
        "  est_is_best_over_t_runs_avgs = np.mean(est_is_best_over_t_runs, axis=0)\n",
        "  plt_est_is_best_over_t_runs_avgs, = axs[1,0].plot(est_is_best_over_t_runs_avgs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
        "\n",
        "  axs[1,0].legend()\n",
        "  axs[1,0].set_xlabel(\"time step\")\n",
        "  axs[1,0].set_ylabel(\"percentage\")\n",
        "  axs[1,0].set_title(\"Percentage of the Estimated Best Action Being the First Action\", y=-0.18)\n",
        "\n",
        "  #plot the mean instantaneous regret over time\n",
        "\n",
        "  l_over_t_runs_avgs = np.mean(l_over_t_runs, axis=0)\n",
        "  axs[1,1].plot(l_over_t_runs_avgs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
        "\n",
        "  axs[1,1].legend()\n",
        "  axs[1,1].set_xlabel(\"time step\")\n",
        "  axs[1,1].set_ylabel(\"regret\")\n",
        "  axs[1,1].set_title(\"Instantaneous Regret over Time\", y=-0.18)\n",
        "\n",
        "  #plot the total regret over time\n",
        "\n",
        "  total_l_over_t_runs_avgs = np.mean(total_l_over_t_runs, axis=0)\n",
        "  axs[2,0].plot(total_l_over_t_runs_avgs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
        "\n",
        "  axs[2,0].legend()\n",
        "  axs[2,0].set_xlabel(\"time step\")\n",
        "  axs[2,0].set_ylabel(\"regret\")\n",
        "  axs[2,0].set_title(\"Total Regret up to Time Step t\", y=-0.18)\n",
        "\n",
        "axs[-1, -1].axis('off')\n",
        "\n",
        "title = r'Graphs  for Epsilon Greedy with Varying Epsilons'\n",
        "fig.suptitle(title, fontsize=16, y=0.08)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILoBgOrocu_z"
      },
      "source": [
        "### Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnrJI7uKAvkH"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qULufDuyXgb"
      },
      "source": [
        "## Q5 Hyperparameters for Epsilon-greedy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVbJAAc0Ebc6"
      },
      "source": [
        "To have a plain start, you have been provided with predefined functions for generating plots until now. However, moving forward, you are expected to plot graphs on your own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH8EgaKmvEbz"
      },
      "source": [
        "### Graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRVYgAJPczbo"
      },
      "source": [
        "### Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzcTHHnbEZbZ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ttd7oJXiQe"
      },
      "source": [
        "## Q6 Gradient Bandit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "Wed5NdLZXjrE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqUx-AYVvu2B"
      },
      "source": [
        "### Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P9ueHpp7Z7e1",
        "outputId": "44651c79-3f0c-4c9b-b173-3f87b6635a3c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NABR2XVSc2fk"
      },
      "source": [
        "### Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw4DpXavIoue"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9GklWhZM9um"
      },
      "source": [
        "## Q7 Thompson Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAlN8q-YM_Mt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG3KAgC5v2gl"
      },
      "source": [
        "### Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tNW238UNONAz",
        "outputId": "1d1a97ce-24c5-4553-a020-8180566829eb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRc45fxdc46B"
      },
      "source": [
        "### Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxB0EtT4MLnO"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggb7m7AwcFb1"
      },
      "source": [
        "## Q8 Comparison of Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMyZZ1P1PNqU"
      },
      "source": [
        "### Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RBo6IK0x6wBJ",
        "outputId": "6aa51e57-03e2-4514-baf2-8420aa9b38fc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwel6MCoc8ms"
      },
      "source": [
        "### Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOWhRBqJP1nP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFBJd8hRHDbj"
      },
      "source": [
        "## Q9 Non-stationary Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "PbNLZxmiSLnP",
        "outputId": "eb55c51e-58f5-4f01-a714-ea1f8dc3dc66"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ5aVJBqc-8j"
      },
      "source": [
        "### Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe6BJtfS8XIq"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
